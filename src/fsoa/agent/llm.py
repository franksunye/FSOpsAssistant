"""
DeepSeek LLMé›†æˆæ¨¡å—

æä¾›ä¸DeepSeek APIçš„é›†æˆï¼Œæ”¯æŒæ™ºèƒ½å†³ç­–å’Œå†…å®¹ç”Ÿæˆ
"""

import json
from typing import Dict, Any, Optional, List
from openai import OpenAI
from datetime import datetime

from ..utils.logger import get_logger
from ..utils.config import get_config
from ..data.models import TaskInfo, DecisionResult, Priority

logger = get_logger(__name__)


class DeepSeekError(Exception):
    """DeepSeekç›¸å…³å¼‚å¸¸"""
    pass


class DeepSeekClient:
    """DeepSeek LLMå®¢æˆ·ç«¯"""
    
    def __init__(self):
        self.config = get_config()
        try:
            # å°è¯•åˆ›å»º OpenAI å®¢æˆ·ç«¯
            self.client = OpenAI(
                api_key=self.config.deepseek_api_key,
                base_url=self.config.deepseek_base_url
            )
        except TypeError as e:
            if "proxies" in str(e):
                # å¦‚æœé‡åˆ° proxies å‚æ•°é”™è¯¯ï¼Œå°è¯•ä½¿ç”¨ç®€åŒ–çš„åˆå§‹åŒ–
                import httpx
                http_client = httpx.Client()
                self.client = OpenAI(
                    api_key=self.config.deepseek_api_key,
                    base_url=self.config.deepseek_base_url,
                    http_client=http_client
                )
            else:
                raise
        
    def analyze_task_priority(self, task: TaskInfo, context: Dict[str, Any] = None) -> DecisionResult:
        """
        åˆ†æä»»åŠ¡ä¼˜å…ˆçº§å’Œå¤„ç†å»ºè®®
        
        Args:
            task: ä»»åŠ¡ä¿¡æ¯
            context: é¢å¤–ä¸Šä¸‹æ–‡ä¿¡æ¯
            
        Returns:
            å†³ç­–ç»“æœ
        """
        try:
            prompt = self._build_priority_analysis_prompt(task, context)

            # ä»æ•°æ®åº“è¯»å–LLMæ¸©åº¦å‚æ•°
            from ..data.database import get_db_manager
            db_manager = get_db_manager()
            temperature_config = db_manager.get_system_config("llm_temperature")
            temperature = float(temperature_config) if temperature_config else 0.1

            response = self.client.chat.completions.create(
                model="deepseek-chat",
                messages=[{"role": "user", "content": prompt}],
                temperature=temperature,
                max_tokens=1000
            )
            
            result_text = response.choices[0].message.content
            result_data = self._parse_decision_result(result_text)
            
            return DecisionResult(
                action=result_data.get("action", "skip"),
                priority=Priority(result_data.get("priority", "normal")),
                message=result_data.get("message"),
                reasoning=result_data.get("reasoning"),
                confidence=result_data.get("confidence", 0.8),
                llm_used=True
            )
            
        except Exception as e:
            logger.error(f"DeepSeek API error: {e}")
            # é™çº§åˆ°è§„åˆ™å†³ç­–
            return self._fallback_rule_decision(task)
    
    def generate_notification_message(self, task: TaskInfo, message_type: str = "overdue_alert") -> str:
        """
        ç”Ÿæˆé€šçŸ¥æ¶ˆæ¯å†…å®¹ - å·²ä¿®æ”¹ä¸ºä½¿ç”¨æ ‡å‡†æ ¼å¼
        
        Args:
            task: ä»»åŠ¡ä¿¡æ¯
            message_type: æ¶ˆæ¯ç±»å‹
            
        Returns:
            ç”Ÿæˆçš„æ¶ˆæ¯å†…å®¹
        """
        # ä¸å†ä½¿ç”¨LLMç”Ÿæˆï¼Œç›´æ¥ä½¿ç”¨æ ‡å‡†æ¨¡æ¿
        logger.info(f"Using standard template for task {task.id} (LLM disabled)")
        return self._fallback_template_message(task, message_type)
    
    def optimize_decision_strategy(self, task: TaskInfo, history: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        åŸºäºå†å²æ•°æ®ä¼˜åŒ–å†³ç­–ç­–ç•¥
        
        Args:
            task: å½“å‰ä»»åŠ¡
            history: å†å²å†³ç­–å’Œç»“æœ
            
        Returns:
            ä¼˜åŒ–å»ºè®®
        """
        try:
            prompt = self._build_strategy_optimization_prompt(task, history)

            # ä»æ•°æ®åº“è¯»å–LLMæ¸©åº¦å‚æ•°ï¼Œç­–ç•¥ä¼˜åŒ–ä½¿ç”¨ç¨é«˜çš„æ¸©åº¦
            from ..data.database import get_db_manager
            db_manager = get_db_manager()
            temperature_config = db_manager.get_system_config("llm_temperature")
            base_temperature = float(temperature_config) if temperature_config else 0.1
            temperature = min(base_temperature + 0.1, 1.0)  # ç­–ç•¥ä¼˜åŒ–ä½¿ç”¨ç¨é«˜çš„æ¸©åº¦

            response = self.client.chat.completions.create(
                model="deepseek-chat",
                messages=[{"role": "user", "content": prompt}],
                temperature=temperature,
                max_tokens=800
            )
            
            result_text = response.choices[0].message.content
            optimization = self._parse_optimization_result(result_text)
            
            logger.info(f"Generated optimization strategy for task {task.id}")
            return optimization
            
        except Exception as e:
            logger.error(f"Failed to optimize strategy: {e}")
            return {"status": "error", "message": str(e)}
    
    def _build_priority_analysis_prompt(self, task: TaskInfo, context: Dict[str, Any] = None) -> str:
        """æ„å»ºä¼˜å…ˆçº§åˆ†ææç¤ºè¯"""
        current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªç°åœºæœåŠ¡è¿è¥ä¸“å®¶ï¼Œè¯·åˆ†æä»¥ä¸‹ä»»åŠ¡çš„ç´§æ€¥ç¨‹åº¦å’Œå¤„ç†å»ºè®®ã€‚

ä»»åŠ¡ä¿¡æ¯ï¼š
- ä»»åŠ¡ID: {task.id}
- ä»»åŠ¡æ ‡é¢˜: {task.title}
- ä»»åŠ¡æè¿°: {task.description or 'æ— '}
- å½“å‰çŠ¶æ€: {task.status.value}
- å½“å‰ä¼˜å…ˆçº§: {task.priority.value}
- SLAæ—¶é—´: {task.sla_hours}å°æ—¶
- å·²ç”¨æ—¶é—´: {task.elapsed_hours:.1f}å°æ—¶
- è¶…æ—¶æ—¶é—´: {task.overdue_hours:.1f}å°æ—¶
- è¶…æ—¶æ¯”ä¾‹: {task.overdue_ratio:.1%}
- è´Ÿè´£äºº: {task.assignee or 'æœªåˆ†é…'}
- å®¢æˆ·: {task.customer or 'æœªçŸ¥'}
- æœåŠ¡åœ°ç‚¹: {task.location or 'æœªçŸ¥'}
- æœ€åé€šçŸ¥æ—¶é—´: {task.last_notification or 'ä»æœª'}

å½“å‰æ—¶é—´: {current_time}

é¢å¤–ä¸Šä¸‹æ–‡: {json.dumps(context or {}, ensure_ascii=False, indent=2)}

è¯·åŸºäºä»¥ä¸Šä¿¡æ¯ï¼Œåˆ†æä»»åŠ¡çš„å¤„ç†ä¼˜å…ˆçº§å’Œå»ºè®®è¡ŒåŠ¨ï¼Œè¿”å›JSONæ ¼å¼ï¼š

{{
    "action": "skip|notify|escalate",
    "priority": "low|normal|high|urgent", 
    "message": "å»ºè®®çš„é€šçŸ¥æ¶ˆæ¯å†…å®¹",
    "reasoning": "å†³ç­–ç†ç”±å’Œåˆ†æè¿‡ç¨‹",
    "confidence": 0.0-1.0
}}

åˆ†æè¦ç‚¹ï¼š
1. è€ƒè™‘è¶…æ—¶ç¨‹åº¦å’Œä¸šåŠ¡å½±å“
2. è¯„ä¼°å®¢æˆ·é‡è¦æ€§å’ŒæœåŠ¡ç´§æ€¥æ€§
3. åˆ†æå†å²é€šçŸ¥é¢‘ç‡ï¼Œé¿å…è¿‡åº¦éªšæ‰°
4. æä¾›å…·ä½“å¯è¡Œçš„è¡ŒåŠ¨å»ºè®®
5. ç¡®ä¿æ¶ˆæ¯å†…å®¹ä¸“ä¸šã€ç®€æ´ã€æœ‰æ•ˆ"""

        return prompt
    
    def _build_message_generation_prompt(self, task: TaskInfo, message_type: str) -> str:
        """æ„å»ºæ¶ˆæ¯ç”Ÿæˆæç¤ºè¯"""
        current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        prompt = f"""è¯·ä¸ºä»¥ä¸‹ç°åœºæœåŠ¡ä»»åŠ¡ç”Ÿæˆä¸€æ¡ä¸“ä¸šçš„{message_type}é€šçŸ¥æ¶ˆæ¯ã€‚

ä»»åŠ¡ä¿¡æ¯ï¼š
- ä»»åŠ¡ID: {task.id}
- ä»»åŠ¡æ ‡é¢˜: {task.title}
- è´Ÿè´£äºº: {task.assignee or 'æœªåˆ†é…'}
- å®¢æˆ·: {task.customer or 'æœªçŸ¥'}
- æœåŠ¡åœ°ç‚¹: {task.location or 'æœªçŸ¥'}
- SLAæ—¶é—´: {task.sla_hours}å°æ—¶
- å·²ç”¨æ—¶é—´: {task.elapsed_hours:.1f}å°æ—¶
- è¶…æ—¶æ—¶é—´: {task.overdue_hours:.1f}å°æ—¶

æ¶ˆæ¯è¦æ±‚ï¼š
1. è¯­è¨€ç®€æ´ä¸“ä¸šï¼Œçªå‡ºé‡ç‚¹
2. åŒ…å«å…³é”®ä¿¡æ¯ï¼ˆä»»åŠ¡IDã€è¶…æ—¶æƒ…å†µã€è´Ÿè´£äººï¼‰
3. æä¾›æ˜ç¡®çš„è¡ŒåŠ¨å»ºè®®
4. è¯­æ°”é€‚ä¸­ï¼Œæ—¢ç´§æ€¥åˆä¸å¤±ç¤¼è²Œ
5. é•¿åº¦æ§åˆ¶åœ¨200å­—ä»¥å†…
6. ä½¿ç”¨é€‚å½“çš„emojiå¢å¼ºå¯è¯»æ€§

è¯·ç›´æ¥è¿”å›æ¶ˆæ¯å†…å®¹ï¼Œä¸éœ€è¦JSONæ ¼å¼ã€‚"""

        return prompt
    
    def _build_strategy_optimization_prompt(self, task: TaskInfo, history: List[Dict[str, Any]]) -> str:
        """æ„å»ºç­–ç•¥ä¼˜åŒ–æç¤ºè¯"""
        prompt = f"""ä½œä¸ºè¿è¥ä¼˜åŒ–ä¸“å®¶ï¼Œè¯·åŸºäºå†å²æ•°æ®åˆ†æå½“å‰ä»»åŠ¡çš„æœ€ä½³å¤„ç†ç­–ç•¥ã€‚

å½“å‰ä»»åŠ¡ï¼š
- ä»»åŠ¡ID: {task.id}
- è¶…æ—¶æƒ…å†µ: {task.overdue_hours:.1f}å°æ—¶
- å®¢æˆ·: {task.customer}
- è´Ÿè´£äºº: {task.assignee}

å†å²æ•°æ®ï¼š
{json.dumps(history, ensure_ascii=False, indent=2)}

è¯·åˆ†æï¼š
1. ç±»ä¼¼ä»»åŠ¡çš„å¤„ç†æ¨¡å¼
2. é€šçŸ¥æ•ˆæœå’Œå“åº”æ—¶é—´
3. æœ€ä½³é€šçŸ¥æ—¶æœºå’Œé¢‘ç‡
4. å‡çº§ç­–ç•¥çš„è§¦å‘æ¡ä»¶

è¿”å›JSONæ ¼å¼çš„ä¼˜åŒ–å»ºè®®ï¼š
{{
    "recommended_action": "å…·ä½“å»ºè®®è¡ŒåŠ¨",
    "optimal_timing": "æœ€ä½³å¤„ç†æ—¶æœº",
    "escalation_threshold": "å‡çº§é˜ˆå€¼å»ºè®®",
    "notification_frequency": "é€šçŸ¥é¢‘ç‡å»ºè®®",
    "success_probability": "æˆåŠŸæ¦‚ç‡è¯„ä¼°",
    "reasoning": "åˆ†æç†ç”±"
}}"""

        return prompt
    
    def _parse_decision_result(self, result_text: str) -> Dict[str, Any]:
        """è§£æå†³ç­–ç»“æœ"""
        try:
            # å°è¯•æå–JSON
            start_idx = result_text.find('{')
            end_idx = result_text.rfind('}') + 1
            
            if start_idx >= 0 and end_idx > start_idx:
                json_str = result_text[start_idx:end_idx]
                return json.loads(json_str)
            else:
                logger.warning("No JSON found in LLM response, using fallback")
                return self._extract_fallback_decision(result_text)
                
        except json.JSONDecodeError as e:
            logger.warning(f"Failed to parse JSON: {e}, using fallback")
            return self._extract_fallback_decision(result_text)
    
    def _parse_optimization_result(self, result_text: str) -> Dict[str, Any]:
        """è§£æä¼˜åŒ–ç»“æœ"""
        try:
            start_idx = result_text.find('{')
            end_idx = result_text.rfind('}') + 1
            
            if start_idx >= 0 and end_idx > start_idx:
                json_str = result_text[start_idx:end_idx]
                return json.loads(json_str)
            else:
                return {"status": "parsed_error", "content": result_text}
                
        except json.JSONDecodeError:
            return {"status": "json_error", "content": result_text}
    
    def _extract_fallback_decision(self, text: str) -> Dict[str, Any]:
        """ä»æ–‡æœ¬ä¸­æå–å†³ç­–ä¿¡æ¯ï¼ˆé™çº§æ–¹æ¡ˆï¼‰"""
        text_lower = text.lower()
        
        # ç®€å•çš„å…³é”®è¯åŒ¹é…
        if "escalate" in text_lower or "å‡çº§" in text_lower:
            action = "escalate"
            priority = "urgent"
        elif "notify" in text_lower or "é€šçŸ¥" in text_lower:
            action = "notify"
            priority = "high" if "urgent" in text_lower or "ç´§æ€¥" in text_lower else "normal"
        else:
            action = "skip"
            priority = "low"
        
        return {
            "action": action,
            "priority": priority,
            "message": "ç³»ç»Ÿè‡ªåŠ¨ç”Ÿæˆçš„æé†’æ¶ˆæ¯",
            "reasoning": "åŸºäºå…³é”®è¯åŒ¹é…çš„é™çº§å†³ç­–",
            "confidence": 0.5
        }
    
    def _fallback_rule_decision(self, task: TaskInfo) -> DecisionResult:
        """è§„åˆ™å†³ç­–é™çº§æ–¹æ¡ˆ"""
        if task.overdue_hours > task.sla_hours:
            action = "escalate"
            priority = Priority.URGENT
        elif task.overdue_hours > 0:
            action = "notify"
            priority = Priority.HIGH
        else:
            action = "skip"
            priority = Priority.NORMAL
        
        return DecisionResult(
            action=action,
            priority=priority,
            message="åŸºäºè§„åˆ™çš„è‡ªåŠ¨å†³ç­–",
            reasoning=f"ä»»åŠ¡è¶…æ—¶{task.overdue_hours:.1f}å°æ—¶ï¼Œè§¦å‘{action}åŠ¨ä½œ",
            confidence=1.0,
            llm_used=False
        )
    
    def _fallback_template_message(self, task: TaskInfo, message_type: str) -> str:
        """æ¨¡æ¿æ¶ˆæ¯é™çº§æ–¹æ¡ˆ"""
        if message_type == "overdue_alert":
            return f"""ğŸš¨ ä»»åŠ¡è¶…æ—¶æé†’

ä»»åŠ¡ID: {task.id}
ä»»åŠ¡æ ‡é¢˜: {task.title}
è´Ÿè´£äºº: {task.assignee or 'æœªåˆ†é…'}
è¶…æ—¶æ—¶é—´: {task.overdue_hours:.1f}å°æ—¶

è¯·åŠæ—¶å¤„ç†ï¼Œç¡®ä¿æœåŠ¡è´¨é‡ï¼"""
        else:
            return f"ä»»åŠ¡ {task.id} éœ€è¦å…³æ³¨ï¼Œè¯·åŠæ—¶å¤„ç†ã€‚"
    
    def test_connection(self) -> bool:
        """æµ‹è¯•DeepSeekè¿æ¥"""
        try:
            response = self.client.chat.completions.create(
                model="deepseek-chat",
                messages=[{"role": "user", "content": "Hello, please respond with 'OK'"}],
                temperature=0,
                max_tokens=10
            )
            
            result = response.choices[0].message.content.strip()
            logger.info("DeepSeek connection test successful")
            return "OK" in result.upper()
            
        except Exception as e:
            logger.error(f"DeepSeek connection test failed: {e}")
            return False


# å…¨å±€å®¢æˆ·ç«¯å®ä¾‹
_deepseek_client: Optional[DeepSeekClient] = None


def get_deepseek_client() -> DeepSeekClient:
    """è·å–DeepSeekå®¢æˆ·ç«¯å®ä¾‹"""
    global _deepseek_client
    if _deepseek_client is None:
        _deepseek_client = DeepSeekClient()
    return _deepseek_client
